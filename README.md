# Lab Text-to-SQL RAG System

A sophisticated RAG (Retrieval Augmented Generation) system that converts natural language questions about lab operations into SQL queries and executes them against your lab database.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Natural       â”‚    â”‚   Vector Store  â”‚    â”‚   Lab Database  â”‚
â”‚   Language      â”‚â”€â”€â”€â–¶â”‚   (Qdrant)      â”‚â”€â”€â”€â–¶â”‚   (Your DB)     â”‚
â”‚   Question      â”‚    â”‚   Schema Search â”‚    â”‚   Query Exec    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   LLM (Ollama)  â”‚
                    â”‚   SQL Generationâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
lab-rag-system/
â”œâ”€â”€ docker-compose.yml          # Docker services configuration
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ api/                        # FastAPI application
â”‚   â”œâ”€â”€ main.py                 # Main application entry point
â”‚   â”œâ”€â”€ config.py               # Configuration settings
â”‚   â”œâ”€â”€ models.py               # Pydantic models
â”‚   â”œâ”€â”€ database.py             # Database connection manager
â”‚   â”œâ”€â”€ vector_store.py         # Qdrant vector store operations
â”‚   â”œâ”€â”€ sql_generator.py        # SQL generation with Ollama
â”‚   â”œâ”€â”€ schema_processor.py     # Schema processing and ingestion
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ Dockerfile              # API container configuration
â”œâ”€â”€ kb/                         # Knowledge base (your table schemas)
â”‚   â”œâ”€â”€ catalog_index.jsonl     # Full table catalog
â”‚   â”œâ”€â”€ o.json                  # POC table schemas
â”‚   â”œâ”€â”€ r.json
â”‚   â”œâ”€â”€ sa.json
â”‚   â”œâ”€â”€ rr.json
â”‚   â”œâ”€â”€ ep.json
â”‚   â”œâ”€â”€ tat.json
â”‚   â”œâ”€â”€ c.json
â”‚   â”œâ”€â”€ cti.json
â”‚   â”œâ”€â”€ m.json
â”‚   â”œâ”€â”€ i.json
â”‚   â”œâ”€â”€ mc.json
â”‚   â”œâ”€â”€ mac.json
â”‚   â”œâ”€â”€ ao.json
â”‚   â”œâ”€â”€ ar.json
â”‚   â”œâ”€â”€ asa.json
â”‚   â”œâ”€â”€ arr.json
â”‚   â””â”€â”€ aep.json
â””â”€â”€ logs/                       # Application logs
```

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
git clone <your-repo>
cd lab-rag-system
```

### 2. Configure Environment

Copy and edit the environment file:
```bash
cp .env.example .env
# Edit .env with your database credentials
```

### 3. Prepare Knowledge Base

Create your table schema files in the `kb/` directory. Each file should contain:

```json
{
  "table_name": "experiments",
  "description": "Laboratory experiments tracking",
  "columns": [
    {
      "name": "experiment_id",
      "type": "INTEGER",
      "description": "Unique experiment identifier",
      "primary_key": true
    },
    {
      "name": "title",
      "type": "VARCHAR(255)",
      "description": "Experiment title"
    },
    {
      "name": "status",
      "type": "VARCHAR(50)",
      "description": "Current status: pending, running, completed, failed"
    },
    {
      "name": "created_at",
      "type": "TIMESTAMP",
      "description": "Experiment creation timestamp"
    }
  ],
  "relationships": [
    "FOREIGN KEY (researcher_id) REFERENCES researchers(id)"
  ],
  "rows": [
    [1, "Protein Analysis", "completed", "2024-01-15T10:30:00"],
    [2, "Cell Culture Study", "running", "2024-01-16T09:15:00"]
  ]
}
```

### 4. Configure Database Driver

Uncomment the appropriate database driver in `api/requirements.txt`:

```bash
# For PostgreSQL
psycopg2-binary==2.9.7

# For MySQL  
mysql-connector-python==8.1.0

# For SQL Server
pyodbc==4.0.39
```

### 5. Start Services

```bash
docker-compose up -d
```

Monitor the startup process:
```bash
docker-compose logs -f
```

### 6. Wait for Model Downloads

First startup takes time as Ollama downloads AI models:
```bash
docker-compose logs -f ollama
```

### 7. Ingest Schema

Once all services are healthy:
```bash
curl -X POST http://localhost:8000/ingest/schema
```

### 8. Test the System

```bash
# Generate SQL without execution
curl -X POST http://localhost:8000/query/sql \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Show me all experiments completed this week",
    "execute_query": false
  }'

# Generate and execute SQL
curl -X POST http://localhost:8000/query/sql \
  -H "Content-Type: application/json" \
  -d '{
    "question": "How many samples were processed today?",
    "execute_query": true,
    "limit": 10
  }'
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `LAB_DB_HOST` | Lab database hostname | `localhost` |
| `LAB_DB_PORT` | Lab database port | `5432` |
| `LAB_DB_NAME` | Lab database name | `lab_db` |
| `LAB_DB_USER` | Database username | `lab_user` |
| `LAB_DB_PASSWORD` | Database password | `lab_password` |
| `LAB_DB_DRIVER` | Database driver | `postgresql` |
| `EMBEDDING_MODEL` | Ollama embedding model | `nomic-embed-text` |
| `CHAT_MODEL` | Ollama chat model | `llama3.2` |

### POC Tables

The system currently supports these 17 tables:
- `o` (Orders)
- `r` (Results)  
- `sa` (Sample Analysis)
- `rr` (Result Reviews)
- `ep` (Experiment Protocols)
- `tat` (Turnaround Time)
- `c` (Customers/Clients)
- `cti` (Customer Test Information)
- `m` (Materials/Methods)
- `i` (Instruments)
- `mc` (Material Consumption)
- `mac` (Machine/Equipment)
- `ao` (Analysis Orders)
- `ar` (Analysis Results)
- `asa` (Analysis Sample Assignment)
- `arr` (Analysis Result Reviews)
- `aep` (Analysis Experiment Protocols)

## ğŸ“š API Endpoints

### Schema Management

- **POST** `/ingest/schema` - Ingest table schemas into vector store
- **GET** `/tables/available` - List available POC tables
- **GET** `/tables/schema/{table_name}` - Get specific table schema
- **DELETE** `/schema/reset` - Reset schema collection

### Query Generation

- **POST** `/query/sql` - Generate SQL from natural language
  ```json
  {
    "question": "Show me all pending experiments",
    "execute_query": true,
    "limit": 50
  }
  ```

### Monitoring

- **GET** `/health` - System health check
- **GET** `/cache/stats` - Query cache statistics
- **DELETE** `/cache/clear` - Clear query cache

## ğŸ§  Example Questions

The system understands various lab-related queries:

### Data Retrieval
- "Show me all experiments completed in the last 7 days"
- "What samples were processed yesterday?"
- "List all pending results waiting for approval"
- "Find experiments by researcher John Smith"

### Analytics
- "How many experiments failed this month?"
- "What's the average processing time for sample analysis?"
- "Which equipment has the highest usage rate?"
- "Show me the success rate by experiment type"

### Monitoring
- "Which samples are overdue for processing?"
- "What equipment needs maintenance?"
- "Show me all critical alerts from today"
- "List experiments that exceeded their scheduled time"

## ğŸ›¡ï¸ Security Features

- **Read-only queries**: Only SELECT statements allowed
- **Query validation**: Blocks dangerous operations (DROP, DELETE, etc.)
- **Table restrictions**: Only POC tables accessible
- **Automatic limits**: Prevents large result sets
- **SQL injection protection**: Input validation and sanitization

## ğŸ” Troubleshooting

### Common Issues

**Schema ingestion fails**
```bash
# Check if KB files exist
ls -la kb/

# Check API logs
docker-compose logs api
```

**Database connection fails**
```bash
# Test database connectivity
docker-compose exec api python -c "
from database import DatabaseManager
from config import settings
import asyncio

async def test():
    db = DatabaseManager(settings.DB_CONFIG)
    result = await db.test_connection()
    print(f'DB Connection: {result}')

asyncio.run(test())
"
```

**Models not downloading**
```bash
# Check Ollama logs
docker-compose logs ollama

# Manually pull models
docker-compose exec ollama ollama pull nomic-embed-text
docker-compose exec ollama ollama pull llama3.2
```

### Log Locations

- API logs: `logs/api.log`
- Docker logs: `docker-compose logs [service]`
- Qdrant logs: `docker-compose logs qdrant`
- Ollama logs: `docker-compose logs ollama`

## ğŸ“ˆ Performance Optimization

### For Large Datasets
- Implement connection pooling (see `database.py`)
- Enable query result caching
- Optimize vector search parameters
- Use more specific table schemas

### For High Volume
- Scale Qdrant with clustering
- Implement API rate limiting
- Add query result pagination
- Monitor resource usage

## ğŸš€ Production Deployment

### Security Checklist
- [ ] Use read-only database credentials
- [ ] Enable HTTPS/TLS
- [ ] Implement authentication
- [ ] Set up monitoring and alerting
- [ ] Configure backup strategies
- [ ] Review query logs regularly

### Scaling Considerations
- Use managed vector database service
- Implement API gateway for load balancing
- Add query analytics and monitoring
- Consider multi-region deployment

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## ğŸ“„ License

[Your License Here]

## ğŸ†˜ Support

For issues and questions:
- Check troubleshooting guide above
- Review API logs in `logs/` directory
- Open an issue with detailed error information